{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from typing import Dict, Callable, List\n",
    "\n",
    "import torch\n",
    "import torch.optim         as optim\n",
    "from torch.distributions   import constraints, transform_to\n",
    "\n",
    "# Pyro probabilistic language for Bayesian hyperparameter tuning\n",
    "#!{sys.executable} -m pip install pyro-ppl\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian hyperparameter tuning\n",
    "The below section adapts the tutorial from the Pyro docs at http://pyro.ai/examples/bo.html. I have used their example as a basis to alter for use with my random forest (and later neural network) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction(Callable):\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def evaluate(self, param_dict: Dict) -> float:\n",
    "        \"\"\"Evaluate the objective function for a supplied set of parameters(param-dict).\n",
    "        Args:\n",
    "            param_dict (Dict): Dictionary of param_name (str): param_values (List[float]) that are used to evaluate \n",
    "            the objective function.\n",
    "        Returns:\n",
    "            float: The value of the objective function at the given point in the parameter space defined by 'param_dict'.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "class BayesParameterOptimiser():\n",
    "    def __init__(self, param_dict: Dict, objective: ObjectiveFunction.evaluate, n_initial_evals: int) -> None:\n",
    "        self._param_dict = param_dict\n",
    "        self._objective = objective\n",
    "        self._n_initial_evals = n_initial_evals\n",
    "        \n",
    "        self._lower_bounds = [min(param_dict[hyperparam]) for hyperparam in param_dict]\n",
    "        self._upper_bounds = [max(param_dict[hyperparam]) for hyperparam in param_dict]\n",
    "        self._parameter_constraints = [constraints.interval(lower, upper) for lower, upper \n",
    "                                       in zip(lower_bounds, upper_bounds)]\n",
    "    \n",
    "        # Generating set of parameters with values randomly selected from the above ranges\n",
    "        X_init = [[options[np.random.randint(len(options))] \n",
    "                   for param, options in self._param_dict.items()] \n",
    "                  for i in range(self._n_initial_evals)]\n",
    "\n",
    "        # Converting to a tensor for compatibility with Pyro and PyTorch\n",
    "        self._X_init = torch.tensor(X_init, dtype = torch.float)\n",
    "        \n",
    "        # Evaluate the objective funtion at each initial point in the hyperparameter space\n",
    "        self._y = self._objective(self._X_init)\n",
    "        print(f'Initial parameters sets: {self._X_init}')\n",
    "        print(f'Objective scores for each initial parameters set: \\n{y}')\n",
    "    \n",
    "        # Defining a Gaussian process prior that will serve as our surrogate objective function\n",
    "        # It is a 'prior' at this stage as it is based on our prior beliefs (value of the objective function at the initial parameter values)\n",
    "        # As each new set of parameters is chosen, the surrogate Gaussian process model will be updated to a posterior based on the new data (following Bayes' theorem)\n",
    "        # The Gaussian process model approximates the distribution over y (the value of the objective function) for a given parameter vector (X)\n",
    "\n",
    "        # Initialising the model\n",
    "        self._gpmodel = gp.models.GPRegression(self._X_init,                       # Tensor of hyperparameter sets\n",
    "                                               self._y,                            # Corresponding validations accuracies\n",
    "                                               gp.kernels.Matern52(input_dim = 1), # Kernel type for the Gaussian process\n",
    "                                               noise = torch.tensor(0.1),          # Variance of Gaussian noise in the GP model\n",
    "                                               jitter = 1.0e-4)                    # Stabilises a key mathematical operation (Cholesky decomposition)\n",
    "        \n",
    "    # Defining a function to find the posterior distribution of the surrogate objective function (the GP model) in light of a new set of parameters\n",
    "    def update_posterior(self, x_new):\n",
    "\n",
    "        # Evaluate the objective function at the new point in parameter space\n",
    "        y = self._objective(x_new)\n",
    "\n",
    "        # Concatenate the new parameter vector (x_new) onto the existing (prior) X space of GP model\n",
    "        X = torch.cat([self._gpmodel.X, x_new])\n",
    "\n",
    "        # Concatenate the corresponding objective function evaluation onto the prior y space\n",
    "        y = torch.cat([self._gpmodel.y, y])\n",
    "\n",
    "        # Update GP model with new expanded parameter and objective spaces\n",
    "        self._gpmodel.set_data(X, y)\n",
    "\n",
    "        # Optimize the GP model's hyperparameters (different to the objective model's parameters!) using Adam with a learning rate of 0.001\n",
    "        self._optimizer = torch.optim.Adam(self._gpmodel.parameters(), lr = 0.001)\n",
    "\n",
    "        # Train the model on the new expanded data\n",
    "        gp.util.train(self._gpmodel, self._optimizer)\n",
    "\n",
    "    # Defining the acquisition function\n",
    "    def lower_confidence_bound(self, x, kappa: int = 2):\n",
    "        \"\"\"This function finds the lower confidence bound of the '_gpmodel' distribution over y for a given x.\n",
    "        The lower confidence bound balances exploitaion (points closer to the mean which are likely to be a maximum)\n",
    "        with exploration (trying points further out in the distribution in the hope of finding a better maximum).\n",
    "        The parameter 'kappa' is a number greater than 0 that sets how interested we are in exploration.\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the mean and variance of the surrogate model\n",
    "        mu, variance = self._gpmodel(x, full_cov = False, noiseless = False)\n",
    "\n",
    "        # Get standard deviation from the variance\n",
    "        sigma = variance.sqrt()\n",
    "\n",
    "        # Return the lower confidence bound, the mean minus kappa times the standard deviation\n",
    "        return mu - (kappa * sigma)\n",
    "\n",
    "    def vector_constrain(self, vector, constraint, inverse = False):\n",
    "\n",
    "        vector = torch.cat(tuple(transform_to(constraint[i]).inv(vector[0][i].float()).view(1) if inverse \n",
    "                                 else transform_to(constraint[i])(vector[0][i].float()).view(1) \n",
    "                                 for i in range(len(vector[0])))).view((vector.shape)).requires_grad_(True)\n",
    "\n",
    "        return vector\n",
    "\n",
    "    # \n",
    "    def find_a_candidate(self, x_init, constraint):\n",
    "\n",
    "        # Transform x to an unconstrained domain\n",
    "        unconstrained_x_init = vector_constrain(x_init, constraint, inverse = True) #transform_to(constraint).inv(x_init)\n",
    "\n",
    "        # Make a gradient enabled clone\n",
    "        unconstrained_x = unconstrained_x_init.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # \n",
    "        minimizer = optim.LBFGS([unconstrained_x], line_search_fn = 'strong_wolfe')\n",
    "\n",
    "        def closure():\n",
    "\n",
    "            minimizer.zero_grad()\n",
    "            x = vector_constrain(unconstrained_x, constraint)\n",
    "            y = lower_confidence_bound(x)\n",
    "            autograd.backward(unconstrained_x, autograd.grad(y, unconstrained_x))\n",
    "\n",
    "            return y\n",
    "\n",
    "        minimizer.step(closure)\n",
    "\n",
    "        # After finding a candidate in the unconstrained domain convert it back to original domain.\n",
    "        x = vector_constrain(unconstrained_x, constraint)\n",
    "\n",
    "        return x.detach()\n",
    "\n",
    "    # \n",
    "    def next_x(self, \n",
    "               constraint, \n",
    "               num_candidates = 5):\n",
    "\n",
    "        candidates = []\n",
    "        values = []\n",
    "\n",
    "        x_init = self._gpmodel.X[-1:]\n",
    "\n",
    "        for i in range(num_candidates):\n",
    "\n",
    "            x = find_a_candidate(x_init, constraint)\n",
    "            y = lower_confidence_bound(x)\n",
    "            candidates.append(x)\n",
    "            values.append(y)\n",
    "            x_init = torch.tensor([[x[0][i].new_empty(1).uniform_(low, high) \n",
    "                                    for i, (low, high) in enumerate(zip(self._lower_bounds, self._upper_bounds))]])\n",
    "\n",
    "        argmin = torch.min(torch.cat(values), dim = 0)[1].item()\n",
    "        return candidates[argmin]\n",
    "    \n",
    "    def optimise(self, n_iterations: int):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self._gpmodel.parameters(), lr = 0.001)\n",
    "        gp.util.train(self._gpmodel, optimizer)\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "\n",
    "            xmin = next_x(self._parameter_constraints)\n",
    "            update_posterior(xmin)\n",
    "\n",
    "            print(f'{i}. Parameters: {self._gpmodel.X[-1:]}, Objective(Parameters): {-self._gpmodel.y[-1:]}')\n",
    "\n",
    "        lin = linear_model.LinearRegression()\n",
    "        lin.fit(np.arange(len(self._gpmodel.y)).reshape(-1, 1), (-self._gpmodel.y) * 100)\n",
    "        plt.plot(np.arange(len(self._gpmodel.y)), (-self._gpmodel.y) * 100)\n",
    "        plt.plot(np.arange(len(self._gpmodel.y)), lin.predict(np.arange(len(self._gpmodel.y)).reshape(-1, 1)), label = 'Linear Fit')\n",
    "        plt.xlabel('Iteration Number')\n",
    "        plt.ylabel('Objective(Parameters)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
